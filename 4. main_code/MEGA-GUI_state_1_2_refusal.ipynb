{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de75244-8c10-4389-91c9-a9be88dda844",
   "metadata": {},
   "source": [
    "The MIT License\n",
    "Copyright (c) 2025 MEGA-GUI\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af796640-9c9a-4f84-aaf3-9d7a33849981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# For VLM\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import base64\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from mimetypes import guess_type\n",
    "\n",
    "\n",
    "def local_image_to_data_url(image_path):\n",
    "    # Guess the MIME type of the image based on the file extension\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = 'application/octet-stream'  # Default MIME type if none is found\n",
    "\n",
    "    # Read and encode the image file\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Construct the data URL\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\"\n",
    "\n",
    "# Load Credentail information\n",
    "credentials_path = '../../../../.credentials.json'\n",
    "with open(credentials_path) as credentials_file:\n",
    "    credentials = json.load(credentials_file)\n",
    "\n",
    "VLLM_SERVER_IP = credentials['VLLM_SERVER_IP']\n",
    "VLLM_SERVER_PORT = credentials['VLLM_SERVER_PORT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc1c13",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12bc05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### Load Benchmark Dataset\n",
    "problem_path =  \"/Data/OSWorld_G/os_world_G_including_byte.parquet\"\n",
    "df = pd.read_parquet(problem_path)\n",
    "idx_list = list(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e31e3",
   "metadata": {},
   "source": [
    "# For vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfeb7088",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS =  {\n",
    "    'Authorization': 'TEST',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "PROXIES = {'http': None, 'https': None, 'no_proxy': VLLM_SERVER_IP}\n",
    "MAX_RETRY = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508de1f",
   "metadata": {},
   "source": [
    "# response parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14f94f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_response(response):\n",
    "    try:\n",
    "        pattern = r'(\\d+)[,\\s]+(\\d+)'\n",
    "        input_string = re.search(pattern, response)\n",
    "\n",
    "        if input_string:\n",
    "            x = int(input_string.group(1))\n",
    "            y = int(input_string.group(2))\n",
    "            return x, y\n",
    "        else:\n",
    "            print(f\"The coordinates could not be found. ::: {response}\")\n",
    "            return -1, -1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during parsing response ::: {e}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b1682-25b7-4a8e-b972-de7233365e08",
   "metadata": {},
   "source": [
    "# Region-Of-Interest (ROI) Deduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa427d",
   "metadata": {},
   "source": [
    "## UI-Tars-72B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ddc1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_UI_TARS_72B_coordinate(image_path, problem_instruction, try_idx=0, temperature=0.0):\n",
    "    #print(\"UI_TARS_72B [START]\", end=\" \")\n",
    "\n",
    "    temp_image = Image.open(image_path)\n",
    "    image_width, image_height = temp_image.size\n",
    "    encoded_image = local_image_to_data_url(image_path)\n",
    "    user_prompt_templete = [f\"{problem_instruction}\", f\"Please complete the following [{problem_instruction}] diligently\", f\"Select Coordinate for the following [{problem_instruction}]\", f\"Click for the following [{problem_instruction}]\"]\n",
    "\n",
    "    for i in range(MAX_RETRY):\n",
    "        try:\n",
    "            prompt_idx = (i+try_idx)%len(user_prompt_templete)\n",
    "            payload = {\n",
    "                \"model\": \"ui-tars-72b\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": user_prompt_templete[prompt_idx]\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"{encoded_image}\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "            time.sleep(1)\n",
    "            response = requests.post(\n",
    "                f\"http://{VLLM_SERVER_IP}:{VLLM_SERVER_PORT}/v1/chat/completions\",\n",
    "                headers=HEADERS,\n",
    "                data=json.dumps(payload),\n",
    "                proxies=PROXIES,\n",
    "                timeout=40\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                x, y = parse_response(response.json()['choices'][0]['message']['content'])\n",
    "                if x == -1 and y == -1:\n",
    "                    return {'answer_x': x, 'answer_y': y}\n",
    "                x = int(image_width* (x/1000))\n",
    "                y = int(image_height* (y/1000))\n",
    "                return {'answer_x': x, 'answer_y': y}\n",
    "        except TimeoutError as e:\n",
    "            time.sleep(1)\n",
    "            print(f\"Timeout occurred : {e}\")\n",
    "        except Exception as e:            \n",
    "            print(f\"Failed to call LLM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63056f92",
   "metadata": {},
   "source": [
    "## Qwen 72B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e7f50cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_TEMPLATE = \"\"\"You are a GUI agent. You are given an instruction and a screenshot. Your job is to output the most relevant point in the screenshot corresponding to the instruction.\n",
    "## Instruction\n",
    "Instruction: {instruction}\n",
    "\n",
    "## Output Format                    \n",
    "(x1, y1)                    \n",
    "where x1, y1 are the coordinates of the target element.\n",
    "\n",
    "## Note\n",
    "- Ensure the chosen coordinate is a valid clickable area\n",
    "- Output only the coordinate of one point in your response.\n",
    "- The screen's resolution is image_width = {image_width} x  image_height = {image_height}.\n",
    "\"\"\"\n",
    "\n",
    "def get_QWEN_72B_coordinate(image_path, problem_instruction, try_idx=0, temperature=0.0):\n",
    "    print(\"QWEN_72B [START]\", end=\" \")\n",
    "    temp_image = Image.open(image_path)\n",
    "    image_width, image_height = temp_image.size\n",
    "    encoded_image = local_image_to_data_url(image_path)\n",
    "    user_prompt_templete_list = [INSTRUCTION_TEMPLATE, f\"Please complete the following [{INSTRUCTION_TEMPLATE}] diligently\", f\"Select Coordinate for the following [{INSTRUCTION_TEMPLATE}]\", f\"Click for the following [{INSTRUCTION_TEMPLATE}]\"]\n",
    "\n",
    "    for i in range(MAX_RETRY):\n",
    "        try:\n",
    "            prompt_idx = (i+try_idx)%len(user_prompt_templete_list)\n",
    "            payload = {\n",
    "                \"model\": \"qwen2.5-vl-72b\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": user_prompt_templete_list[prompt_idx].format(image_width=image_width,image_height=image_height,instruction=problem_instruction)\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"{encoded_image}\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "            time.sleep(1)\n",
    "            response = requests.post(\n",
    "                f\"http://{VLLM_SERVER_IP}:{VLLM_SERVER_PORT}/v1/chat/completions\",\n",
    "                headers=HEADERS,\n",
    "                data=json.dumps(payload),\n",
    "                proxies=PROXIES,\n",
    "                timeout=100\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                x, y = parse_response(response.json()['choices'][0]['message']['content'])\n",
    "                if x == -1 or y == -1:\n",
    "                    print(response.json()['choices'][0]['message']['content'])\n",
    "                    return {'answer_x': -1, 'answer_y': -1}\n",
    "                return {'answer_x': x, 'answer_y': y}\n",
    "        except TimeoutError as e:\n",
    "            time.sleep(1)\n",
    "            print(f\"Timeout occurred : {e}\")\n",
    "        except Exception as e:            \n",
    "            print(f\"Failed to call LLM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630252cc",
   "metadata": {},
   "source": [
    "## CUA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FOR CUA\n",
    "from openai import AzureOpenAI\n",
    "import httpx\n",
    "client_ssl= httpx.Client(verify=False)\n",
    "base_url = f\"{credentials['OPENAI_CUA_API_BASE']}/openai/v1/\"\n",
    "api_key = credentials['OPENAI_CUA_API_KEY']\n",
    "api_type = credentials['OPENAI_API_TYPE']\n",
    "deployment_name = credentials['OPENAI_API_ENGINE_CUA']\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version='preview',\n",
    "    base_url=base_url,\n",
    "    http_client=client_ssl\n",
    ")\n",
    "\n",
    "def get_CUA_coordinate(image_path, problem_instruction, try_idx=0, temperature=0.0):\n",
    "    print(\"CUA [START]\", end=\" \")\n",
    "\n",
    "    temp_image = Image.open(image_path)\n",
    "    image_width, image_height = temp_image.size\n",
    "    encoded_image = local_image_to_data_url(image_path)\n",
    "    \n",
    "    \n",
    "    user_prompt =f\"\"\"You are a GUI agent. You are given an instruction and a screenshot. Your job is to output the most relevant point in the screenshot corresponding to the instruction.\n",
    "## Instruction\n",
    "Instruction: {problem_instruction}\n",
    "\n",
    "## Output Format                    \n",
    "(x1, y1)                    \n",
    "where x1, y1 are the coordinates of the target element.\n",
    "\n",
    "## Note\n",
    "- Ensure the chosen coordinate is a valid clickable area\n",
    "- Output only the coordinate of one point in your response.\"\"\"\n",
    "    messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"input_text\",\n",
    "                            \"text\": user_prompt\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"input_image\",\n",
    "                            \"image_url\": f\"{encoded_image}\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "\n",
    "    for i in range(MAX_RETRY):\n",
    "        try:\n",
    "        \n",
    "            response = client.responses.create(\n",
    "                model=deployment_name,\n",
    "                tools=[{\n",
    "                    \"type\": \"computer_use_preview\",\n",
    "                    \"display_width\": image_width,\n",
    "                    \"display_height\": image_height,\n",
    "                    \"environment\": 'linux'\n",
    "                }],\n",
    "                input=messages,\n",
    "                truncation=\"auto\",\n",
    "                timeout=60,\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.model_dump_json())\n",
    "            response_string = \"(-1, -1)\"\n",
    "            for outputs in result['output']:\n",
    "                if 'action' in outputs :\n",
    "                    response_string = f\"({outputs['action']['x']},{outputs['action']['y']})\"\n",
    "                if 'content' in outputs:\n",
    "                    response_string = outputs['content'][0]['text']\n",
    "            \n",
    "            x, y = parse_response(response_string)\n",
    "            if x == -1 and y == -1:\n",
    "                return {'answer_x': x, 'answer_y': y}\n",
    "        \n",
    "            return {'answer_x': x, 'answer_y': y}\n",
    "        except TimeoutError as e:\n",
    "            time.sleep(1)\n",
    "            print(f\"Timeout occurred : {e}\")\n",
    "        except Exception as e:            \n",
    "            print(f\"Failed to call LLM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a8ac5",
   "metadata": {},
   "source": [
    "## GTA1 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT_GTA1 = f\"\"\"- given role\n",
    "        You are an expert UI element locator..\n",
    "        You should predict the x,y centor position of UI element for user request.\n",
    "        Output format should be only the coordinate x,y single pair exactly:\n",
    "        (x,y)\n",
    "\"\"\"\n",
    "\n",
    "def get_gta1_7b_coordinate(image_path, problem_instruction, try_idx=0, temperature=0.0):\n",
    "    print(\"GTA1_7B [S]\", end=\" \")\n",
    "    temp_image = Image.open(image_path)\n",
    "    image_width, image_height = temp_image.size\n",
    "    encoded_image = local_image_to_data_url(image_path)    \n",
    "\n",
    "    user_prompt_templete = [f\"{problem_instruction}\", f\"Please complete the following [{problem_instruction}] diligently\", f\"Select Coordinate for the following [{problem_instruction}]\", f\"Click for the following [{problem_instruction}]\"]\n",
    "\n",
    "    for i in range(MAX_RETRY):\n",
    "        try:\n",
    "            prompt_idx = (i+try_idx)%len(user_prompt_templete)\n",
    "            chat_history = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": SYS_PROMPT_GTA1}],\n",
    "                }\n",
    "            ]\n",
    "            user_chat = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": f\"find x,y position on {user_prompt_templete[prompt_idx]}\",\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"{encoded_image}\"\n",
    "                                }\n",
    "                            }]\n",
    "            }\n",
    "            chat_history.append(user_chat)\n",
    "            payload = {\n",
    "                \"model\": \"gta1-7b\",\n",
    "                \"messages\": chat_history,\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "            time.sleep(1)\n",
    "            response = requests.post(\n",
    "                f\"http://{VLLM_SERVER_IP}:{VLLM_SERVER_PORT}/v1/chat/completions\",\n",
    "                headers=HEADERS,\n",
    "                data=json.dumps(payload),\n",
    "                proxies=PROXIES,\n",
    "                timeout=100\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                x, y = parse_response(response.json()['choices'][0]['message']['content'])\n",
    "                if x == -1 or y == -1:\n",
    "                    print(response.json()['choices'][0]['message']['content'])\n",
    "                    return {'answer_x': -1, 'answer_y': -1}\n",
    "                return {'answer_x': x, 'answer_y': y}\n",
    "        except TimeoutError as e:\n",
    "            time.sleep(1)\n",
    "            print(f\"Timeout occurred : {e}\")\n",
    "        except Exception as e:            \n",
    "            print(f\"Failed to call LLM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a186c50",
   "metadata": {},
   "source": [
    "## GTA1 72B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT_GTA1 = f\"\"\"- given role\n",
    "        You are an expert UI element locator..\n",
    "        You should predict the x,y centor position of UI element for user request.\n",
    "        Output format should be only the coordinate x,y single pair exactly:\n",
    "        (x,y)\n",
    "\"\"\"\n",
    "\n",
    "def get_gta1_72b_coordinate(image_path, problem_instruction, try_idx=0, temperature=0.0):\n",
    "    print(\"GTA1_72B [S]\", end=\" \")\n",
    "    temp_image = Image.open(image_path)\n",
    "    image_width, image_height = temp_image.size\n",
    "    encoded_image = local_image_to_data_url(image_path)    \n",
    "\n",
    "    user_prompt_templete = [f\"{problem_instruction}\", f\"Please complete the following [{problem_instruction}] diligently\", f\"Select Coordinate for the following [{problem_instruction}]\", f\"Click for the following [{problem_instruction}]\"]\n",
    "\n",
    "    for i in range(MAX_RETRY):\n",
    "        try:\n",
    "            prompt_idx = (i+try_idx)%len(user_prompt_templete)\n",
    "            chat_history = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": SYS_PROMPT_GTA1}],\n",
    "                }\n",
    "            ]\n",
    "            user_chat = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": f\"find x,y position on {user_prompt_templete[prompt_idx]}\",\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"{encoded_image}\"\n",
    "                                }\n",
    "                            }]\n",
    "            }\n",
    "            chat_history.append(user_chat)\n",
    "            payload = {\n",
    "                \"model\": \"gta1-72b\",\n",
    "                \"messages\": chat_history,\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "            time.sleep(1)\n",
    "            response = requests.post(\n",
    "                f\"http://{VLLM_SERVER_IP}:{VLLM_SERVER_PORT}/v1/chat/completions\",\n",
    "                headers=HEADERS,\n",
    "                data=json.dumps(payload),\n",
    "                proxies=PROXIES,\n",
    "                timeout=100\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                x, y = parse_response(response.json()['choices'][0]['message']['content'])\n",
    "                if x == -1 or y == -1:\n",
    "                    print(response.json()['choices'][0]['message']['content'])\n",
    "                    return {'answer_x': -1, 'answer_y': -1}\n",
    "                return {'answer_x': x, 'answer_y': y}\n",
    "        except TimeoutError as e:\n",
    "            time.sleep(1)\n",
    "            print(f\"Timeout occurred : {e}\")\n",
    "        except Exception as e:            \n",
    "            print(f\"Failed to call LLM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad95e2",
   "metadata": {},
   "source": [
    "## Gemini 2.5 pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dbe4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response_for_gemini(response):\n",
    "    try:        \n",
    "        pattern = r'(\\d+)[,\\s]+(\\d+)[,\\s]+(\\d+)[,\\s]+(\\d+)'\n",
    "        input_string = re.search(pattern, response)\n",
    "\n",
    "        if input_string:\n",
    "            y1 = int(input_string.group(1))\n",
    "            x1 = int(input_string.group(2))\n",
    "            y2 = int(input_string.group(3))\n",
    "            x2 = int(input_string.group(4))\n",
    "            x = 0.5 * (x1 + x2)\n",
    "            y = 0.5 * (y1 + y2)\n",
    "            return x, y\n",
    "        else:\n",
    "            print(f\"The coordinates could not be found. ::: {response}\")\n",
    "            return -1, -1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during parsing response ::: {e}\")\n",
    "        pass\n",
    "\n",
    "def get_gemini_coordinate(image_path, problem_instruction, try_idx=0, temperature=0.0):\n",
    "    print(\"G [S]\", end=\" \")\n",
    "    temp_image = Image.open(image_path)\n",
    "    image_width, image_height = temp_image.size\n",
    "    encoded_image = local_image_to_data_url(image_path) \n",
    "\n",
    "\n",
    "    INSTRUCTION_TEMPLATE = \"\"\"You are a GUI agent. You are given an instruction and a screenshot. Your job is to output the most relevant point in the screenshot corresponding to the instruction.\n",
    "## Instruction\n",
    "Instruction: {instruction}\n",
    "\n",
    "## Output Format                    \n",
    "The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\n",
    "\n",
    "## Note\n",
    "- Ensure the chosen coordinate is a valid clickable area\n",
    "- Output only the coordinate of one point in your response.\n",
    "- The screen's resolution is image_width = {image_width} x  image_height = {image_height}.\n",
    "\"\"\"\n",
    "    \n",
    "    user_prompt_templete_list = [INSTRUCTION_TEMPLATE, f\"Please complete the following [{INSTRUCTION_TEMPLATE}] diligently\", f\"Select Coordinate for the following [{INSTRUCTION_TEMPLATE}]\", f\"Click for the following [{INSTRUCTION_TEMPLATE}]\"]\n",
    "\n",
    "    for i in range(MAX_RETRY):\n",
    "        try:\n",
    "            prompt_idx = (i+try_idx)%len(user_prompt_templete_list)\n",
    "            response = requests.post(\n",
    "                url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {credentials['OPENROUTER_API_KEY']}\",\n",
    "                    \"Content-Type\": \"application/json\",\n",
    "                },\n",
    "                data=json.dumps({\n",
    "                    \"model\": \"google/gemini-2.5-pro\",\n",
    "                    \"messages\": [\n",
    "                    {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": user_prompt_templete_list[prompt_idx].format(image_width=image_width,image_height=image_height,instruction=problem_instruction)\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"{encoded_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                    }\n",
    "                ],\n",
    "                })\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                x, y = parse_response_for_gemini(response.json()['choices'][0]['message']['content'])\n",
    "                \n",
    "                if x == -1 and y == -1:\n",
    "                    return {'answer_x': x, 'answer_y': y}\n",
    "                x = int(image_width* (x/1000))\n",
    "                y = int(image_height* (y/1000))\n",
    "                return {'answer_x': x, 'answer_y': y}\n",
    "        except TimeoutError as e:\n",
    "            time.sleep(1)\n",
    "            print(f\"Timeout occurred : {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to call LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8fd51",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7672d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinate(vlm_model_name, image_path, problem_instruction, try_idx=0, temperature=0.0):\n",
    "    if vlm_model_name == \"GEMINIPRO\":\n",
    "        return get_gemini_coordinate(image_path, problem_instruction, try_idx=0, temperature=0.0)\n",
    "    elif vlm_model_name == \"GTA1_7B\":\n",
    "        return get_gta1_7b_coordinate(image_path, problem_instruction, try_idx=0, temperature=0.0)\n",
    "    elif vlm_model_name == \"GTA1_72B\":\n",
    "        return get_gta1_72b_coordinate(image_path, problem_instruction, try_idx=0, temperature=0.0)\n",
    "    elif vlm_model_name == \"CUA\":\n",
    "        return get_CUA_coordinate(image_path, problem_instruction, try_idx=0, temperature=0.0)\n",
    "    elif vlm_model_name == \"UI_TARS_72B\":\n",
    "        return get_UI_TARS_72B_coordinate(image_path, problem_instruction, try_idx=0, temperature=0.0)\n",
    "    elif vlm_model_name == \"QWEN_72B\":\n",
    "        return get_QWEN_72B_coordinate(image_path, problem_instruction, try_idx=0, temperature=0.0)\n",
    "    else:\n",
    "        raise(\"Check VLM (Vision-Language Model) model name\")\n",
    "        \n",
    "\n",
    "def propose_candidate_point(state):\n",
    "    problem = state['problem']\n",
    "    problem_instruction = problem['instruction']\n",
    "    source_image = Image.open(io.BytesIO(problem['image_bytes'])).convert(\"RGBA\")\n",
    "    roi_cropped_image = source_image.copy()\n",
    "\n",
    "    roi_x1, roi_y1, roi_x2, roi_y2 = state['roi_list'][-1]\n",
    "    roi_cropped_image = source_image.crop((roi_x1, roi_y1, roi_x2, roi_y2))\n",
    "    roi_cropped_image_path = state['roi_cropped_image_path']\n",
    "    roi_cropped_image.save(roi_cropped_image_path)    \n",
    "\n",
    "    for try_idx in range(4):        \n",
    "        result = get_coordinate(state['vlm_model_name'], roi_cropped_image_path, problem_instruction, try_idx)\n",
    "        if result is None or result['answer_x'] == -1:\n",
    "            print(f\"answer_x is -1 \")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "    find_coordinate_state = False\n",
    "    if result is not None and 'answer_x' in result:\n",
    "        global_x = result['answer_x'] + roi_x1\n",
    "        global_y = result['answer_y'] + roi_y1        \n",
    "        if roi_x1 <= global_x <= roi_x2 and roi_y1 <= global_y <= roi_y2:\n",
    "            find_coordinate_state = True\n",
    "            \n",
    "    else:\n",
    "        global_x = -1\n",
    "        global_y = -1\n",
    "\n",
    "    candidate_point = {'state': find_coordinate_state, 'x': global_x, 'y': global_y}    \n",
    "    candidate_point_list = state['candidate_point_list']\n",
    "    candidate_point_list.append(candidate_point)\n",
    "\n",
    "    valid_candidate_point_list = state['valid_candidate_point_list']\n",
    "    if find_coordinate_state:\n",
    "        valid_candidate_point_list.append(candidate_point)\n",
    "    \n",
    "    return {'candidate_point_list': candidate_point_list, \"valid_candidate_point_list\": valid_candidate_point_list}\n",
    "\n",
    "\n",
    "\n",
    "def refine_search_area(state):\n",
    "    roi_list = state['roi_list']\n",
    "    problem = state['problem']\n",
    "    source_image = Image.open(io.BytesIO(problem['image_bytes']))\n",
    "    image_width, image_height = source_image.size\n",
    "\n",
    "    candidate_point = state['candidate_point_list'][-1]\n",
    "    cpx = candidate_point['x']\n",
    "    cpy = candidate_point['y']\n",
    "\n",
    "    step_size = state['step_size']\n",
    "    max_zoom_out_count = state['max_zoom_out_count']\n",
    "    \n",
    "    roi_x1, roi_y1, roi_x2, roi_y2 = roi_list[-1]\n",
    "\n",
    "    roi_width = roi_x2 - roi_x1\n",
    "    roi_height = roi_y2 - roi_y1\n",
    "    \n",
    "    # predict coordinate in ROI : Zoom-in \n",
    "    if candidate_point['state']:        \n",
    "        if roi_width > roi_height:\n",
    "            if abs(cpx - roi_x1) > abs(cpx - roi_x2):\n",
    "                roi_x1 += step_size\n",
    "            else:\n",
    "                roi_x2 -= step_size\n",
    "        \n",
    "        else:\n",
    "            if abs(cpy - roi_y1) > abs(cpy - roi_y2):\n",
    "                roi_y1 += step_size\n",
    "            else:\n",
    "                roi_y2 -= step_size\n",
    "    else:\n",
    "        if max_zoom_out_count == 0:\n",
    "            # Correction Zoom-in \n",
    "            if step_size < roi_width:\n",
    "                roi_x1 += step_size//2\n",
    "                roi_x2 -= step_size//2\n",
    "            if step_size < roi_height:\n",
    "                roi_y1 += step_size//2\n",
    "                roi_y2 -= step_size//2\n",
    "        else:\n",
    "            # Zoom-Out\n",
    "            print(f\"\\n ROI {roi_list[-1]} \\n{candidate_point}\")\n",
    "            roi_x1 -= step_size//2\n",
    "            roi_x2 += step_size//2\n",
    "            roi_y1 -= step_size//2\n",
    "            roi_y2 += step_size//2\n",
    "            max_zoom_out_count -= 1\n",
    "\n",
    "    roi_list.append((roi_x1, roi_y1, roi_x2, roi_y2))\n",
    "\n",
    "    return {'roi_list': roi_list,'max_zoom_out_count':max_zoom_out_count}\n",
    "\n",
    "\n",
    "def get_point_dist(candidate_point_a, candidate_point_b):\n",
    "    a_x, a_y = candidate_point_a['x'], candidate_point_a['y']\n",
    "    b_x, b_y = candidate_point_b['x'], candidate_point_b['y']\n",
    "\n",
    "    return math.sqrt((a_x - b_x)**2 + (a_y - b_y)**2)\n",
    "\n",
    "def check_terminate_condition(state):\n",
    "    roi_list = state['roi_list']\n",
    "    roi_x1, roi_y1, roi_x2, roi_y2 = roi_list[-1]\n",
    "    roi_width = roi_x2 - roi_x1\n",
    "    roi_height = roi_y2 - roi_y1\n",
    "    max_length = max(roi_width, roi_height)\n",
    "    step_size = int( state['step_ratio'] * max_length )\n",
    "\n",
    "    target_roi_size = state['target_roi_size']\n",
    "    if roi_width <= target_roi_size and roi_height <= target_roi_size:\n",
    "        center_x = int(0.5*(roi_x2 + roi_x1))\n",
    "        center_y = int(0.5*(roi_y2 + roi_y1))\n",
    "        roi_list[-1] = (\n",
    "            center_x - target_roi_size//2, center_y - target_roi_size//2, \n",
    "            center_x + target_roi_size//2, center_y + target_roi_size//2)\n",
    "\n",
    "    return {'step_size': step_size, 'roi_list' : roi_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5b2bf1-0e1c-4eee-9a35-f5aae7288163",
   "metadata": {},
   "source": [
    "# Fine_Grained Grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4160818d-dce2-4d38-88e0-d6f3ac2b6b07",
   "metadata": {},
   "source": [
    "## setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86066d79-1223-4833-97b4-c12d6497d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import numpy as np\n",
    "\n",
    "def _load_credentials(file_path: str = credentials_path) -> dict:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def _ensure_openai_credentials() -> None:\n",
    "    \"\"\"\n",
    "    Load credentials from file if not in env vars,\n",
    "    then configure openai.\n",
    "    \"\"\"\n",
    "    required_vars = [\n",
    "        \"OPENAI_API_BASE\",\n",
    "        \"OPENAI_API_KEY\",\n",
    "        \"OPENAI_API_TYPE\",\n",
    "        \"OPENAI_API_VERSION\",\n",
    "        \"OPENAI_API_ENGINE_GPT4\",  # or whichever engine names you have\n",
    "        \"OPENAI_API_ENGINE_GPT3\"\n",
    "    ]\n",
    "    if not all(var in os.environ for var in required_vars):\n",
    "        creds = _load_credentials()\n",
    "        for k, v in creds.items():\n",
    "            os.environ[k] = v\n",
    "_ensure_openai_credentials()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6cd0f0-fc64-4c5e-8b89-79428251c005",
   "metadata": {},
   "source": [
    "## LVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8679cf0-431c-40d4-afe8-13527c60a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"OPENAI_API_BASE\"],\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version=\"2024-08-01-preview\",\n",
    "    max_retries=2,\n",
    "    temperature=0.1,\n",
    "    model=\"gpt-4o-1120\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33c671-ecd6-4340-b2d8-48a206a9c26f",
   "metadata": {},
   "source": [
    "## Functions for Fine-Grained Grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63bdc7fa-fd08-403c-ba08-629e227f20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_response(response):\n",
    "    try:\n",
    "        pattern = r'\\((\\d+),\\s*(\\d+)\\)'\n",
    "        input_string = response  \n",
    "\n",
    "        if input_string:\n",
    "            match = re.search(pattern, input_string)\n",
    "            if match:\n",
    "                x = int(match.group(1))\n",
    "                y = int(match.group(2))\n",
    "                return x, y\n",
    "            else:\n",
    "                print(\"No coordinates found\")\n",
    "                raise Exception(\"No coordinates found\")\n",
    "        else:\n",
    "            print(\"Empty response\")\n",
    "            raise Exception(\"Empty response\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó Error: {e}\")\n",
    "        return None\n",
    "        \n",
    "def generate_prompt(step_action):\n",
    "    return f'Output only the precise coordinates (x, y). Find the exact position to click for the action: \"{step_action}\".'\n",
    "        \n",
    "\n",
    "def request_coordinates(instruction, encoded_image):\n",
    "    prompt = generate_prompt(instruction)\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"ui-tars-72b\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \n",
    "                f\"\"\"\n",
    "                You are given a UI screenshot and a precise UI element description.\n",
    "                ### üîç UI Element Description:\n",
    "                {instruction}\n",
    "                ### üìå Instructions:\n",
    "                - ONLY one coordinate pair must be returned.\n",
    "                - The coordinate must be the **center** of the visual UI element (not label text unless it is the clickable part).\n",
    "                - The output format MUST be strictly:\n",
    "                {{ \"x\": <integer>, \"y\": <integer> }}\n",
    "                ### ‚ö†Ô∏è Format Rules:\n",
    "                - Return ONLY valid JSON.\n",
    "                - No extra text, comments, or markdown.\n",
    "                - If the element is not found, return: {{ \"x\": null, \"y\": null }}\n",
    "                Now return the precise coordinates.\n",
    "                \"\"\".strip()\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{encoded_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url=f\"http://{VLLM_SERVER_IP}:{VLLM_SERVER_PORT}/v1/chat/completions\",\n",
    "            headers=HEADERS,\n",
    "            json=data,\n",
    "            proxies=PROXIES,\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            response_text = response.json()['choices'][0]['message']['content'].strip()\n",
    "            coordinates = parse_response(response_text)\n",
    "            return {'answer_x': coordinates[0], 'answer_y': coordinates[1]} if coordinates else None\n",
    "        else:\n",
    "            print(response.text)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è exception: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c843a-314b-4a8f-bc66-1285500e4cf2",
   "metadata": {},
   "source": [
    "## LVM for Fine-Grained grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "853e895c-9af2-4371-9526-9d5290f4af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def image_bytes_to_data_url(image_bytes: bytes, mime_type: str = 'image/png') -> str:\n",
    "    \"\"\"\n",
    "    encodes the image byte and converts it to a data URL.\n",
    "    \"\"\"\n",
    "    base64_encoded_data = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\"\n",
    "\n",
    "def find_application(image_bytes: bytes, raw_instruction: str):    \n",
    "    encoded_image = image_bytes_to_data_url(image_bytes)\n",
    "\n",
    "    messages = [\n",
    "        HumanMessage(content=[\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    f\"\"\"\n",
    "                    You are given a screenshot of a user interface along with a user instruction.\n",
    "                    Your task is to identify which **application or software** this interface belongs to, based on visual clues and the instruction content.\n",
    "                    Please follow these rules:\n",
    "                    1. Look at visible UI elements (e.g., toolbars, menus, icons, window titles) in the screenshot to identify the application.\n",
    "                    2. Use the instruction context to help disambiguate the purpose of the UI.\n",
    "                    3. Be specific. For example, say \"Google Chrome\", \"Microsoft Word\", \"Windows Settings\", \"Adobe Photoshop\", etc.\n",
    "                    4. If you cannot confidently determine the exact application, respond with \"Unknown\".\n",
    "                    Output Format:\n",
    "                    {{\n",
    "                      \"application\": \"<inferred application name>\"\n",
    "                    }}\n",
    "                    Instruction:\n",
    "                    {raw_instruction}\n",
    "                    \"\"\"\n",
    "     \n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": encoded_image}\n",
    "            }\n",
    "        ])\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error while find instruction:\", str(e))\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "        \n",
    "\n",
    "def generate_click_instruction(image_bytes: bytes, raw_instruction: str, app_name: str):\n",
    "    #print(\"[GENERATE CLICK INSTRUCTION in fun]\", \"-\"*10)   \n",
    "\n",
    "    encoded_image = image_bytes_to_data_url(image_bytes)\n",
    "\n",
    "    # LLM message\n",
    "    messages = [\n",
    "        HumanMessage(content=[\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    f\"\"\"\n",
    "                    You are given a screenshot of a user interface and an instruction.\n",
    "                    This screenshot comes from the following application: **{app_name}**\n",
    "                    Your task is to identify the **most relevant and precisely located UI element** (e.g. button, dropdown, input box, icon) that matches the instruction **for performing a click or action**. Focus only on **visible** UI elements.\n",
    "                    To ensure accurate position prediction, follow these detailed rules when generating the description:\n",
    "                    1. Be specific and unambiguous. Avoid vague references like \"icon\", \"window\", or just \"button\". Clearly identify **one unique** UI element.\n",
    "                    2. Describe the UI element in detail:\n",
    "                       - The **element type** (e.g. button, textfield, checkbox, toggle, dropdown).\n",
    "                       - Its **visual characteristics**: shape (e.g. rectangle, circle), color, border style, text label (if any), icons, and other distinctive traits.\n",
    "                       - Its **spatial relationships**: position relative to other visible elements (e.g., \"to the left of the 'Settings' gear icon\", \"below the search bar\").\n",
    "                       - Its **general screen location**: upper/lower/left/right/center part of the screen.\n",
    "                       - The target UI element is guaranteed to be present in the screenshot, usually in the form of single icon.\\n\n",
    "                    3. Clearly distinguish between **similar-looking elements** by referring to nearby labels, icons, or arrangement.\n",
    "                    4. The given instruction does not implicate the name represented on element directly.\n",
    "                    5. Identify a UI element that best matches in order to follow my instruction, specifically focusing to the user action or clickable gui element of the instruction.\n",
    "                    6. Do not speculate or describe elements not visible in the screenshot.\n",
    "                    7. Translate and recognize the Chinese characters that appear on the screen into English.\n",
    "                    8. Only one UI element should be described in the output.\n",
    "                    Return the result in the following JSON format:\n",
    "                    {{\n",
    "                      \"clarified_instruction\": \"<rewritten instruction in one sentence, with ambiguity removed>\",\n",
    "                      \"element_description\": \"<clear, specific description of the UI element including visual and positional info>\",\n",
    "                      \"target_action\": \"click\"                      \n",
    "                    }}\n",
    "                    Instruction:\n",
    "                    {raw_instruction}\n",
    "                    \"\"\"\n",
    "               )\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": encoded_image}\n",
    "            }\n",
    "        ])\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error while generating click instruction:\", str(e))\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec30986-7289-4048-b231-3ad5a04a5534",
   "metadata": {},
   "source": [
    "## Utils for Fine-Grained grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da2fce-f1d9-42cd-867f-fc86b36c9b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from io import BytesIO\n",
    "\n",
    "instruction_counter = defaultdict(int)\n",
    "\n",
    "# sanitize_filename cleans and normalizes a string so it can be safely used as a filename.\n",
    "def sanitize_filename(text):\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    text = re.sub(r'\\s+', '_', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_area_size(target_roi):\n",
    "    roi_x1, roi_y1, roi_x2, roi_y2 = target_roi\n",
    "    roi_width = roi_x2 - roi_x1\n",
    "    roi_height = roi_y2 - roi_y1\n",
    "    return max(roi_width, roi_height)\n",
    "\n",
    "def final_roi_extraction(state):\n",
    "    candidate_point = state['candidate_point_list']\n",
    "    x, y = candidate_point[-1]['x'], candidate_point[-1]['y']\n",
    "    \n",
    "    index = -1\n",
    "    while True:\n",
    "        if candidate_point[index]['state']:\n",
    "            x, y = candidate_point[index]['x'], candidate_point[index]['y']\n",
    "            break\n",
    "        index -= 1\n",
    "\n",
    "    target_roi_size = state['target_roi_size']\n",
    "    roi_x1 = x - (target_roi_size//2)\n",
    "    roi_x2 = x + (target_roi_size//2)    \n",
    "    roi_y1 = y - (target_roi_size//2)\n",
    "    roi_y2 = y + (target_roi_size//2)\n",
    "\n",
    "    roi_self_consistent = [roi_x1, roi_y1, roi_x2, roi_y2]\n",
    "\n",
    "    return {'final_roi': roi_self_consistent}\n",
    "        \n",
    "def fine_grained_grounding(state):\n",
    "    problem = state['problem']\n",
    "    instruction = problem['instruction']\n",
    "    roi_box = state['final_roi']\n",
    "\n",
    "    org_image = Image.open(BytesIO(problem['image_bytes']))\n",
    "    x1_roi, y1_roi, x2_roi, y2_roi = roi_box\n",
    "    roi_image = org_image.crop((x1_roi, y1_roi, x2_roi, y2_roi))\n",
    "    roi_wid, roi_hei = roi_image.size\n",
    "\n",
    "    long_side = max(roi_wid, roi_hei)\n",
    "    scale_v = max(3000 / long_side, 1.0)\n",
    "    up_width = int(roi_wid * scale_v)\n",
    "    up_height = int(roi_hei * scale_v)\n",
    "    upscale_image = roi_image.resize((up_width, up_height), Image.BICUBIC)\n",
    "\n",
    "    # ROI ‚Üí base64 (org scaleup)\n",
    "    buf_raw = BytesIO()\n",
    "    roi_image.save(buf_raw, format=\"PNG\")\n",
    "    encoded_raw = base64.b64encode(buf_raw.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    # Scale Agent\n",
    "    buf_up = BytesIO()\n",
    "    upscale_image.save(buf_up, format=\"PNG\")\n",
    "    encoded_up = base64.b64encode(buf_up.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    case_hits = {}\n",
    "    app_name = \"unknown\"\n",
    "\n",
    "    try:\n",
    "        app_info = find_application(buf_raw.getvalue(), instruction)\n",
    "        if isinstance(app_info, str):\n",
    "            try:\n",
    "                app_info = json.loads(app_info)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Invalid JSON in app_info: {app_info}\")\n",
    "                app_info = {}\n",
    "        app_name = app_info.get(\"application\", \"unknown\")\n",
    "\n",
    "        # Rewrite Agent\n",
    "        new_instruction = generate_click_instruction(buf_raw.getvalue(), instruction, app_name)\n",
    "        # Grounding Agent\n",
    "        coords4 = request_coordinates(new_instruction, encoded_up)\n",
    "        x4n = coords4.get('answer_x', -1)\n",
    "        y4n = coords4.get('answer_y', -1)\n",
    "        x4 = int((x4n * up_width / 1000) / scale_v)\n",
    "        y4 = int((y4n * up_height / 1000) / scale_v)\n",
    "        x_org4 = x4 + x1_roi\n",
    "        y_org4 = y4 + y1_roi\n",
    "        \n",
    "        answer = [x_org4, y_org4]\n",
    "\n",
    "        return {'final_target_prediction': answer} \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Case 4 error: {e}\")\n",
    "\n",
    "    return {'final_target_prediction': [-1,-1]}\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60abbe30-e630-427e-ad40-3a5ba73feb61",
   "metadata": {},
   "source": [
    "# Refusal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c679b9e3-c485-452e-86db-fd32e9dd7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=credentials['OPENROUTER_API_KEY'],\n",
    ")\n",
    "\n",
    "def call_prompt(messages, params):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"google/gemini-2.5-pro\",\n",
    "        messages=messages,\n",
    "        max_tokens= 4096,\n",
    "        temperature=0.1            \n",
    "    )\n",
    "\n",
    "    # print(f\"call_prompt : {completion}\")\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def extract_content_from_response(content):\n",
    "    split_result = content.split(\"```json\")\n",
    "    try:\n",
    "        json_string = split_result[1].replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    except IndexError as E1:\n",
    "        json_string = {\n",
    "            \"answer\": \"no\"\n",
    "        }  \n",
    "    \n",
    "    return json.loads(json_string)\n",
    "\n",
    "def refusal_choice(state):\n",
    "    problem = state['problem']\n",
    "    instruction = problem['instruction']\n",
    "\n",
    "    image = Image.open(BytesIO(problem['image_bytes']))\n",
    "\n",
    "    byte_io = io.BytesIO()\n",
    "    image.save(byte_io, format=\"png\")\n",
    "    encoded_image = base64.b64encode(byte_io.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    SYS_REASONING_PROMPT = \"\"\"\n",
    "            - given role\n",
    "                You are an judge expert to predict whether the next gui action on the given image can excute or not.\n",
    "                So You must analyze whether you have an area to execute user instruction from the given screen.\n",
    "            - given rules to process\n",
    "                You must state the reason for your judgment in the reasoning field.\n",
    "                You must answer 'yes' or 'no' considering reasoning field.\n",
    "            - given rules to judge\n",
    "                If there is even tiny information indicating that user instruction can be executed from the given screen, you must answer 'yes'.\n",
    "                If logically user instruction can't be executed from the given screen, you must answer 'no'.\n",
    "            - given rules to output format\n",
    "                You should return only json format with embrace ```json``` without any comments.\n",
    "                ## format example\n",
    "            ```json\n",
    "            {   \n",
    "                \"reasoning\": \"#your judgement\",\n",
    "                \"answer\": \"#your answer from reasoning\"\n",
    "            }    ```\n",
    "    \"\"\"\n",
    "\n",
    "    params = {'max_tokens': 4096, 'top_p': 0.9, 'temperature': 0.0}    \n",
    "    chat_history = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": SYS_REASONING_PROMPT}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",  \n",
    "                \"content\": [\n",
    "                      {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": f\"{instruction}\"                        \n",
    "                      },\n",
    "                      {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/png;base64,{encoded_image}\"\n",
    "                            },\n",
    "                      }\n",
    "                ]\n",
    "            }\n",
    "    ]\n",
    "\n",
    "    # gemini pro llm Call \n",
    "    response = call_prompt(chat_history, params)\n",
    "    contents = extract_content_from_response(response)\n",
    "\n",
    "    return {'refusal': contents['answer']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459baa6-5b56-478f-804a-847516d63eac",
   "metadata": {},
   "source": [
    "# Graph Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "782f6a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +-----------+                                        \n",
      "              | __start__ |                                        \n",
      "              +-----------+                                        \n",
      "                    *                                              \n",
      "                    *                                              \n",
      "                    *                                              \n",
      "            +----------------+                                     \n",
      "            | refusal_choice |                                     \n",
      "            +----------------+                                     \n",
      "            ...            ...                                     \n",
      "          ..                  ..                                   \n",
      "        ..                      ..                                 \n",
      "+---------+           +-------------------------+                  \n",
      "| __end__ |           | propose_candidate_point |                  \n",
      "+---------+           +-------------------------+                  \n",
      "                         ***                ***                    \n",
      "                      ***                      ***                 \n",
      "                    **                            ***              \n",
      "  +---------------------------+                      **            \n",
      "  | check_terminate_condition |                       *            \n",
      "  +---------------------------+..                     *            \n",
      "                 .               ......               *            \n",
      "                 .                     ......         *            \n",
      "                 .                           ....     *            \n",
      "     +----------------------+              +--------------------+  \n",
      "     | final_roi_extraction |              | refine_search_area |  \n",
      "     +----------------------+              +--------------------+  \n",
      "                 *                                                 \n",
      "                 *                                                 \n",
      "                 *                                                 \n",
      "    +------------------------+                                     \n",
      "    | fine_grained_grounding |                                     \n",
      "    +------------------------+                                     \n",
      "=+=*-*=+=*-*=+=*-*=+=*-*=+=*-*=+=*-*=+=*-*=+=*-*=+=*-*=+=*-*\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tpropose_candidate_point(propose_candidate_point)\n",
      "\trefine_search_area(refine_search_area)\n",
      "\tcheck_terminate_condition(check_terminate_condition)\n",
      "\tfinal_roi_extraction(final_roi_extraction)\n",
      "\tfine_grained_grounding(fine_grained_grounding)\n",
      "\trefusal_choice(refusal_choice)\n",
      "\t__end__(<p>__end__</p>)\n",
      "\t__start__ --> refusal_choice;\n",
      "\tfinal_roi_extraction --> fine_grained_grounding;\n",
      "\tpropose_candidate_point --> check_terminate_condition;\n",
      "\trefine_search_area --> propose_candidate_point;\n",
      "\trefusal_choice -. &nbsp;next&nbsp; .-> propose_candidate_point;\n",
      "\trefusal_choice -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tcheck_terminate_condition -. &nbsp;condition_met&nbsp; .-> final_roi_extraction;\n",
      "\tcheck_terminate_condition -. &nbsp;condition_not_met&nbsp; .-> refine_search_area;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    'GraphState` class inherits from `TypedDict`.\n",
    "\n",
    "    problem field stores the problem.\n",
    "\n",
    "    roi_list field manages a list of regions of interest (ROIs), storing from the original to the Jongno area.\n",
    "    candidate_point_list field stores coordinate results from the VLM for each ROI in a list.\n",
    "    valid_candidate_point_list field stores a list of valid results from the VLM's coordinate inference.\n",
    "\n",
    "    step_size field saves the size to reduce when creating a new ROI.\n",
    "    target_roi_size field determines when to stop if the ROI size is less than or equal to the target size.\n",
    "    max_zoom_out_count field determines the maximum size for zooming out.\n",
    "\n",
    "    step_ratio field saves the ratio at which to reduce the ROI.\n",
    "\n",
    "    vlm_model_name field stores the VLM model name.\n",
    "    roi_cropped_image_path field is the temporary file name used during the process of reducing the ROI.\n",
    "\n",
    "    final_roi Stage 1's roi result\n",
    "    final_target_prediction Stage 2's Final target prediction\n",
    "    \"\"\"\n",
    "    problem: dict\n",
    "\n",
    "    roi_list: list    \n",
    "    candidate_point_list: list\n",
    "    valid_candidate_point_list: list\n",
    "\n",
    "    step_size: int\n",
    "    target_roi_size: int\n",
    "    max_zoom_out_count: int\n",
    "\n",
    "    step_ratio: float\n",
    "\n",
    "    vlm_model_name: str\n",
    "    roi_cropped_image_path: str\n",
    "\n",
    "    final_roi: list\n",
    "    final_target_prediction: list\n",
    "\n",
    "    refusal: str\n",
    "\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(GraphState)\n",
    "\n",
    "### Stage 1\n",
    "graph_builder.add_node(\"propose_candidate_point\", propose_candidate_point)\n",
    "graph_builder.add_node(\"refine_search_area\", refine_search_area)\n",
    "graph_builder.add_node(\"check_terminate_condition\", check_terminate_condition)\n",
    "\n",
    "graph_builder.add_node(\"final_roi_extraction\", final_roi_extraction)\n",
    "\n",
    "### Stage 2\n",
    "graph_builder.add_node(\"fine_grained_grounding\", fine_grained_grounding)\n",
    "\n",
    "### Stage 0\n",
    "graph_builder.add_node(\"refusal_choice\", refusal_choice)\n",
    "\n",
    "\n",
    "def get_search_converged(state):\n",
    "    valid_candidate_point_list = state['valid_candidate_point_list']\n",
    "    if len(valid_candidate_point_list) >= 3:\n",
    "        end_condition = True\n",
    "        recent_candidate_point = valid_candidate_point_list[-1]\n",
    "        for valid_candidate_point in valid_candidate_point_list[-3:-1]:\n",
    "            if get_point_dist(valid_candidate_point, recent_candidate_point) > 50:\n",
    "                end_condition = False\n",
    "        return end_condition\n",
    "\n",
    "def router(state):\n",
    "    target_roi = state['roi_list'][-1]\n",
    "\n",
    "    # Under Min area size\n",
    "    area_size = get_area_size(target_roi)\n",
    "    target_roi_size = state['target_roi_size']\n",
    "    if area_size <= target_roi_size :\n",
    "        return 'condition_met'\n",
    "\n",
    "    # check converge\n",
    "    if get_search_converged(state):\n",
    "        return 'condition_met'\n",
    "\n",
    "    return \"condition_not_met\"\n",
    "\n",
    "def router_refusal(state):\n",
    "    refusal_result = state['refusal']\n",
    "    if refusal_result == 'yes':\n",
    "        return 'next'\n",
    "    else:\n",
    "        return 'end'\n",
    "        \n",
    "graph_builder.add_edge(START,  \"refusal_choice\")\n",
    "graph_builder.add_conditional_edges(\"refusal_choice\", router_refusal,\n",
    "        {\n",
    "            \"next\": \"propose_candidate_point\",\n",
    "            \"end\":END\n",
    "         }\n",
    ")\n",
    "\n",
    "graph_builder.add_edge(\"propose_candidate_point\", \"check_terminate_condition\")\n",
    "graph_builder.add_conditional_edges(\"check_terminate_condition\", router,\n",
    "        {\n",
    "            \"condition_met\": \"final_roi_extraction\",\n",
    "            \"condition_not_met\": \"refine_search_area\"\n",
    "         }\n",
    ")\n",
    "graph_builder.add_edge(\"refine_search_area\", \"propose_candidate_point\")\n",
    "\n",
    "graph_builder.add_edge(\"final_roi_extraction\", \"fine_grained_grounding\")\n",
    "\n",
    "gui_grounding = graph_builder.compile()\n",
    "\n",
    "\n",
    "print(gui_grounding.get_graph().draw_ascii())\n",
    "print(\"=+=*-*\"*10)\n",
    "graph_mmd = 'graph ' + gui_grounding.get_graph().draw_mermaid().split('%%')[-1].strip().split('graph ')[-1]\n",
    "print(graph_mmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e9c33-4cd2-4e76-9815-660444a2a9a1",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# estimate accuracy\n",
    "pass_check_result = dict()\n",
    "# List of ROI(region of interest)\n",
    "roi_list_result = dict()\n",
    "# List of coordinate information for each ROI(region of interest)\n",
    "candidate_point_list_result = dict()\n",
    "\n",
    "# List of final coordinate\n",
    "final_coordinate_result = dict()\n",
    "\n",
    "#VLM_MODEL_NAME = \"GEMINIPRO\"\n",
    "VLM_MODEL_NAME = \"UI_TARS_72B\"\n",
    "\n",
    "step_ratio = 0.1 \n",
    "target_roi_size = 1000\n",
    "max_zoom_out_count = 5\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "prefix = f'OSWORLD_full_{VLM_MODEL_NAME}_{step_ratio}_{target_roi_size}_mzoc_{max_zoom_out_count}'\n",
    "roi_cropped_image_path = f'{prefix}_roi_cropped_image.png'\n",
    "\n",
    "full_count = len(idx_list)\n",
    "start_time = datetime.now()\n",
    "for row_count, idx in enumerate(idx_list):\n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    #print(f\"\\nidx : {idx} ::: row_id : {row_count} :: progress ({100*row_count/full_count}%) :: elasped_time: {elapsed_time}\",  end=\"\\t\")\n",
    "    \n",
    "    # Load each Problem\n",
    "    problem = df.loc[idx]\n",
    "    source_image = Image.open(io.BytesIO(problem['image_bytes']))\n",
    "    width, height = source_image.size\n",
    "\n",
    "    step_size = int( step_ratio * max(width, height) )\n",
    "    result = gui_grounding.invoke({\n",
    "        \"vlm_model_name\" : VLM_MODEL_NAME,\n",
    "        \"problem\": problem, \n",
    "        \"candidate_point_list\": list(), \n",
    "        \"valid_candidate_point_list\": list(), \n",
    "        \"roi_list\": [(0, 0, width, height)], \n",
    "        \"step_size\": step_size,\n",
    "        \"step_ratio\": step_ratio,\n",
    "        \"max_zoom_out_count\": max_zoom_out_count,\n",
    "        \"target_roi_size\": target_roi_size,\n",
    "        \"roi_cropped_image_path\": roi_cropped_image_path,\n",
    "        \"final_target_prediction\": [-1, -1]},        \n",
    "        {\"recursion_limit\": 100000})\n",
    "\n",
    "    gt_box = problem['bbox']\n",
    "    gt_x1 = gt_box[0] * width\n",
    "    gt_y1 = gt_box[1] * height\n",
    "    gt_x2 = gt_box[2] * width\n",
    "    gt_y2 = gt_box[3] * height\n",
    "\n",
    "    # for negative sampling\n",
    "    bbox = [gt_x1, gt_y1, gt_x2, gt_y2]\n",
    "\n",
    "    roi_list = result['roi_list']\n",
    "\n",
    "    find_zoom_area_check_list = list()\n",
    "    pass_check_list = list()\n",
    "\n",
    "    candidate_point_list = result['candidate_point_list']\n",
    "    for c_p_idx, candidate_point in enumerate(candidate_point_list):        \n",
    "        cpx, cpy = candidate_point['x'], candidate_point['y']\n",
    "\n",
    "        if candidate_point['state'] == False:\n",
    "            continue\n",
    "\n",
    "        if gt_x1 <= cpx <= gt_x2 and gt_y1 <= cpy <= gt_y2:\n",
    "            pass_check_list.append(1)\n",
    "        else:\n",
    "            pass_check_list.append(0)\n",
    "        \n",
    "\n",
    "    #print(f\"\\n{pass_check_list}\", end=\"\")\n",
    "\n",
    "\n",
    "    key_name = f\"{prefix}_{idx}\"\n",
    "\n",
    "    #print(result['refusal'])\n",
    "    roi_list_result[key_name] = roi_list\n",
    "    candidate_point_list_result[key_name] = candidate_point_list\n",
    "    final_coordinate_result[key_name] = result['final_target_prediction']\n",
    "    print(f\"idx : {idx}    final coordinate: {result['final_target_prediction']}\")\n",
    "\n",
    "    #if idx > 1:\n",
    "        #break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d984a12-b4a4-4b62-9bd7-4267891d65c5",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6bae161-2b3a-4651-9923-0bb673906402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anser: {0: True, 1: False, 2: True}\n",
      "Number of correct answer: 2\n",
      "\n",
      "ROI: {0: True, 1: True, 2: True}\n",
      "Number of correct ROI: 3\n"
     ]
    }
   ],
   "source": [
    "hit_cnt = 0 ## the number of correct answer\n",
    "fine_grained_grounding_result = dict() ## answer\n",
    "ROI_result = dict() ##ROI result\n",
    "ROI_hit_cnt = 0 ## the number of correct ROI\n",
    "\n",
    "def calculate_overlap_ratio(box_a, box_b):\n",
    "    # Box A Coordinate\n",
    "    x1_a, y1_a, x2_a, y2_a = box_a\n",
    "    # Box B Coordinate\n",
    "    x1_b, y1_b, x2_b, y2_b = box_b\n",
    "\n",
    "    # overlap\n",
    "    x_overlap = max(0, min(x2_a, x2_b) - max(x1_a, x1_b))\n",
    "    y_overlap = max(0, min(y2_a, y2_b) - max(y1_a, y1_b))\n",
    "\n",
    "    # overlapped area\n",
    "    overlap_area = x_overlap * y_overlap\n",
    "\n",
    "    if overlap_area == 0:\n",
    "        return 0\n",
    "    return overlap_area/((x2_a - x1_a) * (y2_a-y1_a))\n",
    "\n",
    "for idx in idx_list:\n",
    "    #if idx > 2:\n",
    "        #break\n",
    "\n",
    "    key_name = f\"{prefix}_{idx}\"\n",
    "    problem = df.loc[idx]\n",
    "\n",
    "    gt_box = problem['bbox']\n",
    "    gt_x1 = gt_box[0] * width\n",
    "    gt_y1 = gt_box[1] * height\n",
    "    gt_x2 = gt_box[2] * width\n",
    "    gt_y2 = gt_box[3] * height\n",
    "    gt_bbox = [gt_x1, gt_y1, gt_x2, gt_y2]\n",
    "\n",
    "    ### answer evaluation\n",
    "    if gt_x1 <= final_coordinate_result[key_name][0] <= gt_x2 and gt_y1 <= final_coordinate_result[key_name][1] <= gt_y2:\n",
    "        hit_cnt += 1\n",
    "        fine_grained_grounding_result[idx] = True\n",
    "    else:\n",
    "        fine_grained_grounding_result[idx] = False    \n",
    "\n",
    "    ### ROI evaluation\n",
    "    roi_x1, roi_y1, roi_x2, roi_y2 = roi_list_result[key_name][-1]\n",
    "    roi_region = [roi_x1, roi_y1, roi_x2, roi_y2]\n",
    "    if calculate_overlap_ratio(gt_bbox, roi_region) >= 0.8:\n",
    "        ROI_hit_cnt += 1\n",
    "        ROI_result[idx] = True\n",
    "    else:\n",
    "        ROI_result[idx] = False \n",
    "    \n",
    "    \n",
    "print(f\"anser: {fine_grained_grounding_result}\") ### True or False for each problem\n",
    "print(f\"Number of correct answer: {hit_cnt}\")  ### Number of correct answer\n",
    "print()\n",
    "print(f\"ROI: {ROI_result}\") ### True or False for each problem\n",
    "print(f\"Number of correct ROI: {ROI_hit_cnt}\")  ### Number of correct answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b8e0be",
   "metadata": {},
   "source": [
    "### Save ROI_LIST & CANDIDATE_POINT_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Stage 1 result's\n",
    "with open(f'roi_list_result_{prefix}.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(roi_list_result, file)\n",
    "with open(f'candidate_point_list_result_{prefix}.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(candidate_point_list_result, file)\n",
    "\n",
    "# Stage 2 result's\n",
    "with open(f'fine_grained_grounding_result_{prefix}.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(fine_grained_grounding_result, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
